{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import os\n",
    "import lda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "n_top_words = 20\n",
    "n_features = 6000\n",
    "n_iter = 500\n",
    "#n_sample = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "19278\n",
      "done in 0.147s.\n"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "f_lst = ['new_clean_biology.csv', 'new_clean_cooking.csv','new_clean_crypto.csv',\n",
    "        'new_clean_diy.csv','new_clean_robotics.csv','new_clean_travel.csv']\n",
    "print('loading dataset...')\n",
    "t0 = time()\n",
    "f = f_lst[5]\n",
    "dataset = pd.read_csv(path+f, header=0)\n",
    "data_sample = dataset['question']#.iloc[:n_sample]\n",
    "tag_sample = dataset['tags']#.iloc[:n_sample]\n",
    "n_sample = len(data_sample.index)\n",
    "print(n_sample)\n",
    "print('done in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TfIdfVectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                  ngram_range=(1,1), \n",
    "                                  min_df=0, \n",
    "                                  stop_words='english')\n",
    "matrix = TfIdfVectorizer.fit_transform(data_sample)\n",
    "feature_names = TfIdfVectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tf features for LDA...\n",
      "done in 1.068s.\n"
     ]
    }
   ],
   "source": [
    "print('extracting tf features for LDA...')\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features,\n",
    "                               stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_sample)\n",
    "print('done in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 19278\n",
      "INFO:lda:vocab_size: 6000\n",
      "INFO:lda:n_words: 816405\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<0> log likelihood: -9622323\n",
      "INFO:lda:<10> log likelihood: -7005140\n",
      "INFO:lda:<20> log likelihood: -6642906\n",
      "INFO:lda:<30> log likelihood: -6497020\n",
      "INFO:lda:<40> log likelihood: -6423438\n",
      "INFO:lda:<50> log likelihood: -6387314\n",
      "INFO:lda:<60> log likelihood: -6364565\n",
      "INFO:lda:<70> log likelihood: -6347697\n",
      "INFO:lda:<80> log likelihood: -6333139\n",
      "INFO:lda:<90> log likelihood: -6324904\n",
      "INFO:lda:<100> log likelihood: -6316666\n",
      "INFO:lda:<110> log likelihood: -6310219\n",
      "INFO:lda:<120> log likelihood: -6304970\n",
      "INFO:lda:<130> log likelihood: -6300032\n",
      "INFO:lda:<140> log likelihood: -6295761\n",
      "INFO:lda:<150> log likelihood: -6292635\n",
      "INFO:lda:<160> log likelihood: -6290116\n",
      "INFO:lda:<170> log likelihood: -6286801\n",
      "INFO:lda:<180> log likelihood: -6282958\n",
      "INFO:lda:<190> log likelihood: -6281784\n",
      "INFO:lda:<200> log likelihood: -6279282\n",
      "INFO:lda:<210> log likelihood: -6278329\n",
      "INFO:lda:<220> log likelihood: -6276400\n",
      "INFO:lda:<230> log likelihood: -6274189\n",
      "INFO:lda:<240> log likelihood: -6272822\n",
      "INFO:lda:<250> log likelihood: -6270960\n",
      "INFO:lda:<260> log likelihood: -6270140\n",
      "INFO:lda:<270> log likelihood: -6266938\n",
      "INFO:lda:<280> log likelihood: -6266690\n",
      "INFO:lda:<290> log likelihood: -6264424\n",
      "INFO:lda:<300> log likelihood: -6263579\n",
      "INFO:lda:<310> log likelihood: -6264983\n",
      "INFO:lda:<320> log likelihood: -6262484\n",
      "INFO:lda:<330> log likelihood: -6263242\n",
      "INFO:lda:<340> log likelihood: -6260351\n",
      "INFO:lda:<350> log likelihood: -6261562\n",
      "INFO:lda:<360> log likelihood: -6259039\n",
      "INFO:lda:<370> log likelihood: -6258795\n",
      "INFO:lda:<380> log likelihood: -6258774\n",
      "INFO:lda:<390> log likelihood: -6257518\n",
      "INFO:lda:<400> log likelihood: -6256751\n",
      "INFO:lda:<410> log likelihood: -6257273\n",
      "INFO:lda:<420> log likelihood: -6258214\n",
      "INFO:lda:<430> log likelihood: -6255749\n",
      "INFO:lda:<440> log likelihood: -6255581\n",
      "INFO:lda:<450> log likelihood: -6253878\n",
      "INFO:lda:<460> log likelihood: -6254939\n",
      "INFO:lda:<470> log likelihood: -6252092\n",
      "INFO:lda:<480> log likelihood: -6253118\n",
      "INFO:lda:<490> log likelihood: -6253589\n",
      "INFO:lda:<499> log likelihood: -6251779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 50.795s.\n"
     ]
    }
   ],
   "source": [
    "print('fitting LDA...')\n",
    "t0 = time()\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=n_iter, random_state=1)\n",
    "model.fit(tf)\n",
    "topic_word = model.topic_word_\n",
    "topic_word_dict = {}\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(feature_names)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    topic_word_dict[i] = topic_words\n",
    "#     print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "print('done in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caribbean cruising vacations (top topic:) 15\n",
      "15\n",
      "guides extreme-tourism amazon-river amazon-jungle (top topic:) 14\n",
      "14\n",
      "loyalty-programs routes ewr singapore-airlines sin (top topic:) 2\n",
      "2\n",
      "romania transportation (top topic:) 14\n",
      "14\n",
      "extreme-tourism antarctica (top topic:) 1\n",
      "1\n",
      "usa airport-transfer taxis seattle (top topic:) 11\n",
      "17\n",
      "sightseeing public-transport transportation argentina (top topic:) 3\n",
      "3\n",
      "safety international-travel money exchange (top topic:) 4\n",
      "4\n",
      "russia visas china mongolia trans-siberian (top topic:) 12\n",
      "12\n",
      "online-resources transportation peru south-america bolivia (top topic:) 9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(0, 10):\n",
    "    print(\"{} (top topic:) {}\".format(tag_sample[i], doc_topic[i].argmax()))\n",
    "    print(doc_topic[i].argsort()[::-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "all_word_file = open(f[10:-4]+ '_all.csv', 'w')\n",
    "clean_word_file = open(f[10:-4]+ '_cleaned.csv', 'w')\n",
    "fieldnames_all = ['doc', 'topic_words']\n",
    "fieldnames_clean = ['doc', 'cleaned_topic_words']\n",
    "writer1 = csv.DictWriter(all_word_file, fieldnames = fieldnames_all)\n",
    "writer1.writeheader()\n",
    "writer2 = csv.DictWriter(clean_word_file, fieldnames = fieldnames_clean)\n",
    "writer2.writeheader()\n",
    "\n",
    "for n in range(n_sample):\n",
    "    doc = dataset['doc'][n]\n",
    "    question = dataset['question'][n]\n",
    "    top_3 = doc_topic[n].argsort()[::-1][:3]\n",
    "    topic_words = list(topic_word_dict[top_3[0]]) + list(topic_word_dict[top_3[1]]) + list(topic_word_dict[top_3[2]])\n",
    "    clean_w = []\n",
    "    for item in topic_words:\n",
    "        if item in question:\n",
    "            clean_w.append(item)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    writer1.writerow({'doc': doc, 'topic_words':' '.join(topic_words)})\n",
    "    writer2.writerow({'doc': doc, 'cleaned_topic_words':' '.join(clean_w)})\n",
    "\n",
    "\n",
    "all_word_file.close()\n",
    "clean_word_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
